import arxiv
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
import time
import random
from config import Config

# Seleniumæ”¯æŒï¼ˆå¯é€‰ï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


class Paper:
    """è®ºæ–‡æ•°æ®ç±»"""
    
    def __init__(self, title: str, abstract: str, url: str, pdf_url: Optional[str] = None,
                 authors: List[str] = None, published: Optional[str] = None, source: str = ""):
        self.title = title
        self.abstract = abstract
        self.url = url
        self.pdf_url = pdf_url
        self.authors = authors or []
        self.published = published
        self.source = source
        
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'title': self.title,
            'abstract': self.abstract,
            'url': self.url,
            'pdf_url': self.pdf_url,
            'authors': self.authors,
            'published': self.published,
            'source': self.source
        }


class SearchEngine:
    """å­¦æœ¯æœç´¢å¼•æ“åŸºç±»"""
    
    def __init__(self):
        # ä¸åœ¨åˆå§‹åŒ–æ—¶å›ºå®šmax_resultsï¼Œæ”¹ä¸ºæ¯æ¬¡æœç´¢æ—¶åŠ¨æ€è·å–
        pass
    
    @property
    def max_results(self):
        """åŠ¨æ€è·å–æœ€å¤§ç»“æœæ•°"""
        from config import Config
        return Config.MAX_RESULTS
        
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """æœç´¢è®ºæ–‡"""
        raise NotImplementedError


class ArxivSearchEngine(SearchEngine):
    """ArXivæœç´¢å¼•æ“"""
    
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """
        åœ¨ArXivä¸Šæœç´¢è®ºæ–‡
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            # æ„å»ºæŸ¥è¯¢
            query = keywords
            
            # åˆ›å»ºæœç´¢å®¢æˆ·ç«¯
            client = arxiv.Client()
            search = arxiv.Search(
                query=query,
                max_results=self.max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            # æ‰§è¡Œæœç´¢
            for result in client.results(search):
                # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                published_date = result.published.strftime('%Y-%m-%d')
                
                if start_date and published_date < start_date:
                    continue
                if end_date and published_date > end_date:
                    continue
                
                paper = Paper(
                    title=result.title,
                    abstract=result.summary,
                    url=result.entry_id,
                    pdf_url=result.pdf_url,
                    authors=[author.name for author in result.authors],
                    published=published_date,
                    source="ArXiv"
                )
                papers.append(paper)
                
        except Exception as e:
            print(f"ArXivæœç´¢å‡ºé”™: {str(e)}")
            
        return papers


class OpenReviewSearchEngine(SearchEngine):
    """OpenReviewæœç´¢å¼•æ“"""
    
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """
        åœ¨OpenReviewä¸Šæœç´¢è®ºæ–‡
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            # OpenReview V2 æœç´¢API
            url = "https://api2.openreview.net/notes/search"
            params = {
                'term': keywords,
                'limit': min(self.max_results, 100),
                'offset': 0
            }
            
            response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                notes = data.get('notes', [])
                
                for note in notes[:self.max_results]:
                    content = note.get('content', {})
                    
                    # æå–æ—¥æœŸ (V2 APIæ ¼å¼)
                    cdate = note.get('cdate', 0)
                    if cdate:
                        published_date = datetime.fromtimestamp(cdate / 1000).strftime('%Y-%m-%d')
                    else:
                        published_date = None
                    
                    # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                    if published_date:
                        if start_date and published_date < start_date:
                            continue
                        if end_date and published_date > end_date:
                            continue
                    
                    # V2 APIçš„contentç»“æ„ä¸åŒï¼Œå­—æ®µå¯èƒ½æ˜¯å¯¹è±¡
                    def get_value(field):
                        """ä»V2 APIçš„å­—æ®µä¸­æå–å€¼"""
                        if isinstance(field, dict):
                            return field.get('value', '')
                        return field if field else ''
                    
                    title = get_value(content.get('title', ''))
                    if not title or title == 'No Title':
                        # è·³è¿‡æ²¡æœ‰æ ‡é¢˜çš„è®ºæ–‡ï¼ˆé€šå¸¸æ˜¯è¯„è®ºæˆ–å…¶ä»–éæ­£å¼å†…å®¹ï¼‰
                        continue
                    
                    abstract = get_value(content.get('abstract', ''))
                    if not abstract:
                        # å°è¯•ä»å…¶ä»–å­—æ®µè·å–æ‘˜è¦
                        abstract = get_value(content.get('summary', ''))
                    if not abstract:
                        abstract = 'No Abstract'
                    
                    authors = content.get('authors', [])
                    if isinstance(authors, dict):
                        authors = authors.get('value', [])
                    if not isinstance(authors, list):
                        authors = []
                    
                    note_id = note.get('id', '')
                    
                    paper = Paper(
                        title=title,
                        abstract=abstract,
                        url=f"https://openreview.net/forum?id={note_id}",
                        pdf_url=f"https://openreview.net/pdf?id={note_id}",
                        authors=authors,
                        published=published_date,
                        source="OpenReview"
                    )
                    papers.append(paper)
            else:
                print(f"OpenReview APIå“åº”é”™è¯¯: {response.status_code}")
                    
        except Exception as e:
            print(f"OpenReviewæœç´¢å‡ºé”™: {str(e)}")
            
        return papers


class GoogleScholarSearchEngine(SearchEngine):
    """Google Scholaræœç´¢å¼•æ“ï¼ˆSeleniumä¼˜å…ˆï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    
    def __init__(self):
        super().__init__()
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        self.use_selenium = SELENIUM_AVAILABLE
    
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """æœç´¢è®ºæ–‡ï¼ˆä¼˜å…ˆä½¿ç”¨Seleniumï¼‰"""
        papers = []
        
        # ä¼˜å…ˆä½¿ç”¨Selenium
        if self.use_selenium and SELENIUM_AVAILABLE:
            print("ğŸš€ ä½¿ç”¨Seleniumæµè§ˆå™¨æ¨¡æ‹Ÿæœç´¢...")
            papers = self._search_with_selenium(keywords, start_date, end_date)
            if papers:
                return papers
            print("âš ï¸ Seleniumæœç´¢å¤±è´¥")
        else:
            print("âš ï¸ Seleniumä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install selenium webdriver-manager")
            print("ğŸ’¡ æˆ–è€…ä½¿ç”¨ArXivå’ŒOpenReviewä½œä¸ºæ›¿ä»£æ•°æ®æº")
        
        return papers
    
    def _search_with_selenium(self, keywords: str, 
                             start_date: Optional[str] = None,
                             end_date: Optional[str] = None) -> List[Paper]:
        """ä½¿ç”¨Seleniumæ¨¡æ‹Ÿæµè§ˆå™¨æœç´¢"""
        papers = []
        driver = None
        
        try:
            print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
            
            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless=new')  # æ–°ç‰ˆæ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-software-rasterizer')
            chrome_options.add_argument(f'user-agent={random.choice(self.user_agents)}')
            
            # åæ£€æµ‹è®¾ç½®
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ä»£ç†è®¾ç½®ï¼ˆå¦‚æœéœ€è¦ï¼Œå–æ¶ˆæ³¨é‡Šï¼‰
            # chrome_options.add_argument('--proxy-server=http://127.0.0.1:7890')
            
            # åˆå§‹åŒ–æµè§ˆå™¨
            try:
                service = Service(ChromeDriverManager().install())
                driver = webdriver.Chrome(service=service, options=chrome_options)
            except Exception as e:
                print(f"âš ï¸ ChromeDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print("ğŸ’¡ å°è¯•ä½¿ç”¨ç³»ç»ŸChrome...")
                driver = webdriver.Chrome(options=chrome_options)
            
            # è®¾ç½®è„šæœ¬é˜²æ­¢è¢«æ£€æµ‹ä¸ºè‡ªåŠ¨åŒ–
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                '''
            })
            
            driver.set_page_load_timeout(30)
            
            # æ„å»ºURL
            query = keywords.replace(" ", "+")
            url = f"https://scholar.google.com/scholar?q={query}&hl=zh-CN&num={min(20, self.max_results)}"
            
            if start_date:
                url += f"&as_ylo={start_date[:4]}"
            if end_date:
                url += f"&as_yhi={end_date[:4]}"
            
            print(f"ğŸ” æ­£åœ¨è®¿é—®: {url[:80]}...")
            
            # è®¿é—®é¡µé¢
            driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(random.uniform(3, 5))
            
            # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            page_source = driver.page_source.lower()
            
            if 'sorry' in page_source or 'unusual traffic' in page_source:
                print("âš ï¸ Googleæ£€æµ‹åˆ°å¼‚å¸¸æµé‡ï¼Œéœ€è¦éªŒè¯")
                print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
                print("   1. ç­‰å¾…10-15åˆ†é’Ÿåé‡è¯•")
                print("   2. ä½¿ç”¨VPN/ä»£ç†ï¼ˆå–æ¶ˆä»£ç ä¸­çš„proxy-serveræ³¨é‡Šï¼‰")
                print("   3. ä¸´æ—¶ä½¿ç”¨ArXivå’ŒOpenReview")
                return papers
            
            if 'captcha' in page_source:
                print("âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ")
                print("ğŸ’¡ å»ºè®®å¯ç”¨æœ‰å¤´æ¨¡å¼ï¼ˆæ³¨é‡Šæ‰--headlessï¼‰æ‰‹åŠ¨å®ŒæˆéªŒè¯")
                return papers
            
            # è·å–é¡µé¢æºç 
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # è§£æç»“æœ
            results = soup.find_all(class_="gs_ri")
            
            if not results:
                print("âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœ")
                # ä¿å­˜HTMLç”¨äºè°ƒè¯•
                # with open('debug_scholar.html', 'w', encoding='utf-8') as f:
                #     f.write(html)
                return papers
            
            print(f"ğŸ“„ æ‰¾åˆ° {len(results)} ä¸ªæœç´¢ç»“æœï¼Œå¼€å§‹è§£æ...")
            
            for idx, result in enumerate(results[:min(len(results), self.max_results)], 1):
                try:
                    title_elem = result.find('h3')
                    if not title_elem:
                        continue
                    
                    paper = Paper(
                        title="",
                        abstract="",
                        url="",
                        pdf_url=None,
                        authors=[],
                        published=None,
                        source="Google Scholar"
                    )
                    
                    # æ ‡é¢˜
                    paper.title = title_elem.get_text().strip()
                    paper.title = paper.title.replace('[HTML]', '').replace('[PDF]', '').replace('[å›¾ä¹¦]', '').strip()
                    
                    # é“¾æ¥
                    link = title_elem.find('a')
                    if link and link.has_attr('href'):
                        paper.url = link.get('href')
                    
                    # æ‘˜è¦
                    abstract_elem = result.find(class_="gs_rs")
                    if abstract_elem:
                        paper.abstract = abstract_elem.get_text().strip()
                    else:
                        paper.abstract = "æ‘˜è¦ä¸å¯ç”¨"
                    
                    # æœŸåˆŠ/ä½œè€…
                    journal_elem = result.find(class_="gs_a")
                    if journal_elem:
                        paper.published = journal_elem.get_text()
                    
                    # å°è¯•æå–PDFé“¾æ¥
                    pdf_links = result.find_all('a', href=True)
                    for link in pdf_links:
                        href = link.get('href', '')
                        if '.pdf' in href.lower() and href.startswith('http'):
                            paper.pdf_url = href
                            break
                    
                    papers.append(paper)
                    
                except Exception as e:
                    print(f"âš ï¸ è§£æç¬¬{idx}ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            if papers:
                print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            else:
                print("âš ï¸ æœªèƒ½è§£æå‡ºä»»ä½•è®ºæ–‡")
            
        except Exception as e:
            print(f"âš ï¸ Seleniumæœç´¢å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
        
        return papers
    


class SearchManager:
    """æœç´¢ç®¡ç†å™¨ï¼Œç»Ÿä¸€ç®¡ç†å¤šä¸ªæœç´¢å¼•æ“"""
    
    def __init__(self):
        self.engines = {
            'arxiv': ArxivSearchEngine(),
            'openreview': OpenReviewSearchEngine(),
            'google_scholar': GoogleScholarSearchEngine()
        }
        
    def search_all(self, keywords: str, start_date: Optional[str] = None,
                   end_date: Optional[str] = None, sources: List[str] = None) -> List[Paper]:
        """
        åœ¨æ‰€æœ‰é€‰å®šçš„æœç´¢å¼•æ“ä¸Šæœç´¢
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            sources: è¦æœç´¢çš„æ¥æºåˆ—è¡¨ï¼Œé»˜è®¤å…¨éƒ¨
            
        Returns:
            æ‰€æœ‰æœç´¢ç»“æœçš„åˆå¹¶åˆ—è¡¨
        """
        if sources is None:
            sources = list(self.engines.keys())
            
        all_papers = []
        
        for source in sources:
            if source in self.engines:
                print(f"æ­£åœ¨æœç´¢ {source}...")
                papers = self.engines[source].search(keywords, start_date, end_date)
                all_papers.extend(papers)
                print(f"ä» {source} æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                
        return all_papers


# åˆ›å»ºå…¨å±€æœç´¢ç®¡ç†å™¨å®ä¾‹
search_manager = SearchManager()
