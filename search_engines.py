import arxiv
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
import time
import random
from config import Config

# Seleniumæ”¯æŒï¼ˆå¯é€‰ï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


class Paper:
    """è®ºæ–‡æ•°æ®ç±»"""
    
    def __init__(self, title: str, abstract: str, url: str, pdf_url: Optional[str] = None,
                 authors: List[str] = None, published: Optional[str] = None, source: str = ""):
        self.title = title
        self.abstract = abstract
        self.url = url
        self.pdf_url = pdf_url
        self.authors = authors or []
        self.published = published
        self.source = source
        
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'title': self.title,
            'abstract': self.abstract,
            'url': self.url,
            'pdf_url': self.pdf_url,
            'authors': self.authors,
            'published': self.published,
            'source': self.source
        }


class SearchEngine:
    """å­¦æœ¯æœç´¢å¼•æ“åŸºç±»"""
    
    def __init__(self):
        # ä¸åœ¨åˆå§‹åŒ–æ—¶å›ºå®šmax_resultsï¼Œæ”¹ä¸ºæ¯æ¬¡æœç´¢æ—¶åŠ¨æ€è·å–
        pass
    
    @property
    def max_results(self):
        """åŠ¨æ€è·å–æœ€å¤§ç»“æœæ•°"""
        from config import Config
        return Config.MAX_RESULTS
        
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """æœç´¢è®ºæ–‡"""
        raise NotImplementedError


class ArxivSearchEngine(SearchEngine):
    """ArXivæœç´¢å¼•æ“"""
    
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """
        åœ¨ArXivä¸Šæœç´¢è®ºæ–‡
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            # æ„å»ºæŸ¥è¯¢
            query = keywords
            
            # åˆ›å»ºæœç´¢å®¢æˆ·ç«¯
            client = arxiv.Client()
            search = arxiv.Search(
                query=query,
                max_results=self.max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            # æ‰§è¡Œæœç´¢
            for result in client.results(search):
                # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                published_date = result.published.strftime('%Y-%m-%d')
                
                if start_date and published_date < start_date:
                    continue
                if end_date and published_date > end_date:
                    continue
                
                paper = Paper(
                    title=result.title,
                    abstract=result.summary,
                    url=result.entry_id,
                    pdf_url=result.pdf_url,
                    authors=[author.name for author in result.authors],
                    published=published_date,
                    source="ArXiv"
                )
                papers.append(paper)
                
        except Exception as e:
            print(f"ArXivæœç´¢å‡ºé”™: {str(e)}")
            
        return papers


class OpenReviewSearchEngine(SearchEngine):
    """OpenReviewæœç´¢å¼•æ“"""
    
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """
        åœ¨OpenReviewä¸Šæœç´¢è®ºæ–‡
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            # OpenReview V2 æœç´¢API
            url = "https://api2.openreview.net/notes/search"
            params = {
                'term': keywords,
                'limit': min(self.max_results, 100),
                'offset': 0
            }
            
            response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                notes = data.get('notes', [])
                
                for note in notes[:self.max_results]:
                    content = note.get('content', {})
                    
                    # æå–æ—¥æœŸ (V2 APIæ ¼å¼)
                    cdate = note.get('cdate', 0)
                    if cdate:
                        published_date = datetime.fromtimestamp(cdate / 1000).strftime('%Y-%m-%d')
                    else:
                        published_date = None
                    
                    # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                    if published_date:
                        if start_date and published_date < start_date:
                            continue
                        if end_date and published_date > end_date:
                            continue
                    
                    # V2 APIçš„contentç»“æ„ä¸åŒï¼Œå­—æ®µå¯èƒ½æ˜¯å¯¹è±¡
                    def get_value(field):
                        """ä»V2 APIçš„å­—æ®µä¸­æå–å€¼"""
                        if isinstance(field, dict):
                            return field.get('value', '')
                        return field if field else ''
                    
                    title = get_value(content.get('title', ''))
                    if not title or title == 'No Title':
                        # è·³è¿‡æ²¡æœ‰æ ‡é¢˜çš„è®ºæ–‡ï¼ˆé€šå¸¸æ˜¯è¯„è®ºæˆ–å…¶ä»–éæ­£å¼å†…å®¹ï¼‰
                        continue
                    
                    abstract = get_value(content.get('abstract', ''))
                    if not abstract:
                        # å°è¯•ä»å…¶ä»–å­—æ®µè·å–æ‘˜è¦
                        abstract = get_value(content.get('summary', ''))
                    if not abstract:
                        abstract = 'No Abstract'
                    
                    authors = content.get('authors', [])
                    if isinstance(authors, dict):
                        authors = authors.get('value', [])
                    if not isinstance(authors, list):
                        authors = []
                    
                    note_id = note.get('id', '')
                    
                    paper = Paper(
                        title=title,
                        abstract=abstract,
                        url=f"https://openreview.net/forum?id={note_id}",
                        pdf_url=f"https://openreview.net/pdf?id={note_id}",
                        authors=authors,
                        published=published_date,
                        source="OpenReview"
                    )
                    papers.append(paper)
            else:
                print(f"OpenReview APIå“åº”é”™è¯¯: {response.status_code}")
                    
        except Exception as e:
            print(f"OpenReviewæœç´¢å‡ºé”™: {str(e)}")
            
        return papers


class GoogleScholarSearchEngine(SearchEngine):
    """Google Scholaræœç´¢å¼•æ“ï¼ˆSeleniumä¼˜å…ˆï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    
    def __init__(self):
        super().__init__()
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        self.use_selenium = SELENIUM_AVAILABLE
    
    def search(self, keywords: str, start_date: Optional[str] = None, 
               end_date: Optional[str] = None) -> List[Paper]:
        """æœç´¢è®ºæ–‡ï¼ˆä¼˜å…ˆä½¿ç”¨Seleniumï¼‰"""
        papers = []
        
        # ä¼˜å…ˆä½¿ç”¨Selenium
        if self.use_selenium and SELENIUM_AVAILABLE:
            print("ğŸš€ ä½¿ç”¨Seleniumæµè§ˆå™¨æ¨¡æ‹Ÿæœç´¢...")
            papers = self._search_with_selenium(keywords, start_date, end_date)
            if papers:
                return papers
            print("âš ï¸ Seleniumæœç´¢å¤±è´¥")
        else:
            print("âš ï¸ Seleniumä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install selenium webdriver-manager")
            print("ğŸ’¡ æˆ–è€…ä½¿ç”¨ArXivå’ŒOpenReviewä½œä¸ºæ›¿ä»£æ•°æ®æº")
        
        return papers
    
    def _search_with_selenium(self, keywords: str, 
                             start_date: Optional[str] = None,
                             end_date: Optional[str] = None) -> List[Paper]:
        """ä½¿ç”¨Seleniumæ¨¡æ‹Ÿæµè§ˆå™¨æœç´¢"""
        papers = []
        driver = None
        
        try:
            print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
            
            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            # chrome_options.add_argument('--headless=new')  # æ³¨é‡Šæ‰æ— å¤´æ¨¡å¼ï¼Œå¯ç”¨å¯è§†åŒ–æµè§ˆå™¨
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-software-rasterizer')
            chrome_options.add_argument(f'user-agent={random.choice(self.user_agents)}')
            
            # åæ£€æµ‹è®¾ç½®
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ä»£ç†è®¾ç½®ï¼ˆå¦‚æœéœ€è¦ï¼Œå–æ¶ˆæ³¨é‡Šï¼‰
            chrome_options.add_argument('--proxy-server=http://127.0.0.1:7890')
            
            # åˆå§‹åŒ–æµè§ˆå™¨
            try:
                service = Service(ChromeDriverManager().install())
                driver = webdriver.Chrome(service=service, options=chrome_options)
            except Exception as e:
                print(f"âš ï¸ ChromeDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print("ğŸ’¡ å°è¯•ä½¿ç”¨ç³»ç»ŸChrome...")
                driver = webdriver.Chrome(options=chrome_options)
            
            # è®¾ç½®è„šæœ¬é˜²æ­¢è¢«æ£€æµ‹ä¸ºè‡ªåŠ¨åŒ–
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                '''
            })
            
            driver.set_page_load_timeout(30)
            
            # æ„å»ºURL
            query = keywords.replace(" ", "+")
            url = f"https://scholar.google.com/scholar?q={query}&hl=zh-CN&num={min(20, self.max_results)}"
            
            if start_date:
                url += f"&as_ylo={start_date[:4]}"
            if end_date:
                url += f"&as_yhi={end_date[:4]}"
            
            print(f"ğŸ” æ­£åœ¨è®¿é—®: {url[:80]}...")
            
            # è®¿é—®é¡µé¢
            driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(random.uniform(3, 5))
            
            # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            page_source = driver.page_source.lower()
            
            if 'sorry' in page_source or 'unusual traffic' in page_source:
                print("âš ï¸ Googleæ£€æµ‹åˆ°å¼‚å¸¸æµé‡ï¼Œéœ€è¦äººå·¥éªŒè¯")
                print("ğŸŒ æµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯...")
                print("â³ ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯ï¼ˆæœ€å¤š120ç§’ï¼‰...")
                
                # ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯ï¼ˆæ£€æŸ¥URLæ˜¯å¦æ”¹å˜æˆ–é¡µé¢å†…å®¹æ˜¯å¦æ”¹å˜ï¼‰
                max_wait = 120  # æœ€å¤šç­‰å¾…120ç§’
                start_time = time.time()
                verified = False
                
                while time.time() - start_time < max_wait:
                    try:
                        current_source = driver.page_source.lower()
                        # æ£€æŸ¥æ˜¯å¦å·²ç»é€šè¿‡éªŒè¯ï¼ˆéªŒè¯é¡µé¢æ¶ˆå¤±ï¼‰
                        if 'sorry' not in current_source and 'unusual traffic' not in current_source:
                            if 'scholar' in driver.current_url and 'gs_ri' in driver.page_source:
                                print("âœ… éªŒè¯æˆåŠŸï¼ç»§ç»­æœç´¢...")
                                verified = True
                                break
                        time.sleep(2)  # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
                    except:
                        pass
                
                if not verified:
                    print("â° éªŒè¯è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
                    return papers
                
                # éªŒè¯æˆåŠŸåé‡æ–°è·å–é¡µé¢å†…å®¹
                time.sleep(2)
                page_source = driver.page_source.lower()
            
            if 'captcha' in page_source and 'gs_ri' not in driver.page_source:
                print("âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œéœ€è¦äººå·¥éªŒè¯")
                print("ğŸŒ æµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯...")
                print("â³ ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯ï¼ˆæœ€å¤š120ç§’ï¼‰...")
                
                # ç­‰å¾…éªŒè¯ç å®Œæˆ
                max_wait = 120
                start_time = time.time()
                verified = False
                
                while time.time() - start_time < max_wait:
                    try:
                        current_source = driver.page_source.lower()
                        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœç´¢ç»“æœ
                        if 'gs_ri' in driver.page_source and 'captcha' not in current_source:
                            print("âœ… éªŒè¯æˆåŠŸï¼ç»§ç»­æœç´¢...")
                            verified = True
                            break
                        time.sleep(2)
                    except:
                        pass
                
                if not verified:
                    print("â° éªŒè¯è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
                    return papers
                
                # éªŒè¯æˆåŠŸåé‡æ–°è·å–é¡µé¢å†…å®¹
                time.sleep(2)
            
            # è·å–é¡µé¢æºç 
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # å°è¯•ç‚¹å‡»"æ›´å¤š"æŒ‰é’®å±•å¼€æ‰€æœ‰æ‘˜è¦
            try:
                # æŸ¥æ‰¾å¹¶ç‚¹å‡»æ‰€æœ‰"æ˜¾ç¤ºæ›´å¤š"æŒ‰é’®
                show_more_buttons = driver.find_elements(By.CLASS_NAME, 'gs_rs')
                for button_elem in show_more_buttons[:5]:  # åªå±•å¼€å‰5ä¸ªé¿å…è¶…æ—¶
                    try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰"..."è¡¨ç¤ºè¢«æˆªæ–­
                        if '...' in button_elem.text:
                            # å°è¯•ç‚¹å‡»å±•å¼€
                            driver.execute_script("arguments[0].click();", button_elem)
                            time.sleep(0.5)
                    except:
                        pass
                
                # é‡æ–°è·å–é¡µé¢å†…å®¹
                time.sleep(1)
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
            except Exception as e:
                print(f"  â„¹ï¸ æ— æ³•å±•å¼€æ‘˜è¦: {str(e)}")
            
            # è§£æç»“æœ
            results = soup.find_all(class_="gs_ri")
            
            if not results:
                print("âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœ")
                # ä¿å­˜HTMLç”¨äºè°ƒè¯•
                # with open('debug_scholar.html', 'w', encoding='utf-8') as f:
                #     f.write(html)
                return papers
            
            print(f"ğŸ“„ æ‰¾åˆ° {len(results)} ä¸ªæœç´¢ç»“æœï¼Œå¼€å§‹è§£æ...")
            
            for idx, result in enumerate(results[:min(len(results), self.max_results)], 1):
                try:
                    title_elem = result.find('h3')
                    if not title_elem:
                        continue
                    
                    paper = Paper(
                        title="",
                        abstract="",
                        url="",
                        pdf_url=None,
                        authors=[],
                        published=None,
                        source="Google Scholar"
                    )
                    
                    # æ ‡é¢˜
                    paper.title = title_elem.get_text().strip()
                    paper.title = paper.title.replace('[HTML]', '').replace('[PDF]', '').replace('[å›¾ä¹¦]', '').strip()
                    
                    # é“¾æ¥
                    link = title_elem.find('a')
                    if link and link.has_attr('href'):
                        paper.url = link.get('href')
                    
                    # æ‘˜è¦ - è·å–å®Œæ•´æ‘˜è¦ï¼ˆåŒ…æ‹¬è¢«éšè—çš„éƒ¨åˆ†ï¼‰
                    abstract_elem = result.find(class_="gs_rs")
                    if abstract_elem:
                        # è·å–æ‰€æœ‰æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¯èƒ½è¢«æŠ˜å çš„å†…å®¹
                        full_abstract = abstract_elem.get_text(separator=' ', strip=True)
                        paper.abstract = full_abstract
                        
                        # å¦‚æœæ‘˜è¦ä»¥"..."ç»“å°¾ï¼Œè¯´æ˜è¢«æˆªæ–­äº†
                        if paper.abstract.endswith('...') or len(paper.abstract) < 150:
                            # å¯¹äºarXivè®ºæ–‡ï¼Œç›´æ¥ä»arXivè·å–å®Œæ•´æ‘˜è¦
                            if paper.url and 'arxiv.org' in paper.url:
                                print(f"  ğŸ”„ è®ºæ–‡{idx}æ‘˜è¦è¢«æˆªæ–­ï¼Œä»arXivè·å–å®Œæ•´ç‰ˆ...")
                                enhanced_abstract = self._fetch_full_abstract(paper.url, driver)
                                if enhanced_abstract and len(enhanced_abstract) > len(paper.abstract):
                                    paper.abstract = enhanced_abstract
                                    print(f"  âœ… è·å–åˆ°å®Œæ•´æ‘˜è¦: {len(paper.abstract)} å­—ç¬¦")
                        
                        print(f"  ğŸ“ è®ºæ–‡{idx}æ‘˜è¦: {paper.abstract[:100]}{'...' if len(paper.abstract) > 100 else ''}")
                    else:
                        paper.abstract = "æ‘˜è¦ä¸å¯ç”¨"
                    
                    # ä½œè€…å’Œå‡ºç‰ˆä¿¡æ¯
                    authors_elem = result.find(class_="gs_a")
                    if authors_elem:
                        author_info = authors_elem.get_text().strip()
                        paper.published = author_info
                        # å°è¯•æå–ä½œè€…åç§°
                        if ' - ' in author_info:
                            authors_part = author_info.split(' - ')[0]
                            paper.authors = [a.strip() for a in authors_part.split(',')]
                    
                    # æå–PDFé“¾æ¥ - æ”¹è¿›ç­–ç•¥
                    # 1. é¦–å…ˆæŸ¥æ‰¾å³ä¾§çš„PDFé“¾æ¥ï¼ˆé€šå¸¸åœ¨gs_or_ggsmç±»ä¸­ï¼‰
                    pdf_link_elem = result.find_parent(class_='gs_r').find(class_='gs_or_ggsm') if result.find_parent(class_='gs_r') else None
                    if pdf_link_elem:
                        pdf_a = pdf_link_elem.find('a', href=True)
                        if pdf_a and pdf_a.get('href'):
                            href = pdf_a.get('href')
                            if href.startswith('http'):
                                paper.pdf_url = href
                    
                    # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨ç»“æœä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ…å«PDFçš„é“¾æ¥
                    if not paper.pdf_url:
                        all_links = result.find_parent(class_='gs_r').find_all('a', href=True) if result.find_parent(class_='gs_r') else result.find_all('a', href=True)
                        for link_elem in all_links:
                            href = link_elem.get('href', '')
                            link_text = link_elem.get_text().lower()
                            # æŸ¥æ‰¾æ˜ç¡®æ ‡æ³¨ä¸ºPDFçš„é“¾æ¥
                            if ('[pdf]' in link_text or 'pdf' in link_text) and href.startswith('http'):
                                paper.pdf_url = href
                                break
                            # æˆ–è€…é“¾æ¥ç›´æ¥æŒ‡å‘PDFæ–‡ä»¶
                            elif '.pdf' in href.lower() and href.startswith('http'):
                                paper.pdf_url = href
                                break
                    
                    # 3. æ™ºèƒ½PDFæŸ¥æ‰¾ï¼šå¦‚æœè®ºæ–‡URLæ˜¯arXivã€Semantic Scholarç­‰ï¼Œå°è¯•æ„å»ºPDFé“¾æ¥
                    if not paper.pdf_url and paper.url:
                        paper.pdf_url = self._try_construct_pdf_url(paper.url)
                    
                    # è°ƒè¯•ä¿¡æ¯
                    pdf_status = "âœ…" if paper.pdf_url else "âŒ"
                    print(f"  {pdf_status} è®ºæ–‡{idx}: {paper.title[:50]}...")
                    
                    papers.append(paper)
                    
                except Exception as e:
                    print(f"âš ï¸ è§£æç¬¬{idx}ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            if papers:
                print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            else:
                print("âš ï¸ æœªèƒ½è§£æå‡ºä»»ä½•è®ºæ–‡")
            
        except Exception as e:
            print(f"âš ï¸ Seleniumæœç´¢å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
        
        return papers
    
    def _try_construct_pdf_url(self, url: str) -> Optional[str]:
        """å°è¯•ä»è®ºæ–‡URLæ„å»ºPDFé“¾æ¥"""
        if not url:
            return None
        
        try:
            # arXiv: å°†absé“¾æ¥è½¬æ¢ä¸ºpdfé“¾æ¥
            if 'arxiv.org/abs/' in url:
                return url.replace('/abs/', '/pdf/') + '.pdf'
            
            # Semantic Scholar
            if 'semanticscholar.org/paper/' in url:
                # Semantic Scholarçš„PDFéœ€è¦é€šè¿‡APIæˆ–é‡å®šå‘è·å–ï¼Œè¿™é‡Œå…ˆè¿”å›None
                pass
            
            # ACM Digital Library
            if 'dl.acm.org' in url and '/doi/' in url:
                # ACMçš„PDFéœ€è¦è®¢é˜…ï¼Œè¿”å›None
                pass
            
            # IEEE Xplore
            if 'ieeexplore.ieee.org' in url:
                # IEEEçš„PDFéœ€è¦è®¢é˜…ï¼Œè¿”å›None
                pass
                
        except Exception as e:
            print(f"âš ï¸ æ„å»ºPDFé“¾æ¥å¤±è´¥: {str(e)}")
        
        return None
    
    def _fetch_full_abstract(self, url: str, driver) -> Optional[str]:
        """ä»è®ºæ–‡åŸå§‹é¡µé¢è·å–å®Œæ•´æ‘˜è¦"""
        if not url or not url.startswith('http'):
            return None
        
        try:
            # å¯¹äºarXivé“¾æ¥ï¼Œä½¿ç”¨ç‰¹æ®Šå¤„ç†
            if 'arxiv.org/abs/' in url:
                current_window = driver.current_window_handle
                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[-1])
                
                try:
                    driver.get(url)
                    time.sleep(2)
                    
                    # arXivçš„æ‘˜è¦åœ¨blockquote.abstractå…ƒç´ ä¸­
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    abstract_elem = soup.find('blockquote', class_='abstract')
                    if abstract_elem:
                        # ç§»é™¤"Abstract:"æ ‡ç­¾
                        abstract_text = abstract_elem.get_text(strip=True)
                        abstract_text = abstract_text.replace('Abstract:', '').strip()
                        return abstract_text
                finally:
                    driver.close()
                    driver.switch_to.window(current_window)
            
            # å¯¹äºå…¶ä»–é“¾æ¥ï¼Œå°è¯•é€šç”¨æ–¹æ³•ï¼ˆé™åˆ¶é¿å…è¿‡åº¦è¯·æ±‚ï¼‰
            # è¿™é‡Œæˆ‘ä»¬æš‚æ—¶ä¸å¤„ç†ï¼Œé¿å…æ‰“å¼€å¤ªå¤šé¡µé¢å½±å“æ€§èƒ½
            
        except Exception as e:
            print(f"    âš ï¸ è·å–å®Œæ•´æ‘˜è¦å¤±è´¥: {str(e)}")
        
        return None


class SearchManager:
    """æœç´¢ç®¡ç†å™¨ï¼Œç»Ÿä¸€ç®¡ç†å¤šä¸ªæœç´¢å¼•æ“"""
    
    def __init__(self):
        self.engines = {
            'arxiv': ArxivSearchEngine(),
            'openreview': OpenReviewSearchEngine(),
            'google_scholar': GoogleScholarSearchEngine()
        }
    
    def _filter_paper(self, paper: Paper, exclude_keywords: list, require_keywords: list) -> bool:
        """æ™ºèƒ½è¿‡æ»¤è®ºæ–‡
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡
            exclude_keywords: æ’é™¤å…³é”®è¯åˆ—è¡¨
            require_keywords: å¿…éœ€å…³é”®è¯åˆ—è¡¨
            
        Returns:
            Trueè¡¨ç¤ºä¿ç•™ï¼ŒFalseè¡¨ç¤ºè¿‡æ»¤æ‰
        """
        # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦ç”¨äºæ£€æŸ¥
        content = (paper.title + ' ' + paper.abstract).lower()
        
        # æ£€æŸ¥æ’é™¤å…³é”®è¯
        if exclude_keywords:
            for keyword in exclude_keywords:
                if keyword.lower() in content:
                    print(f"  ğŸš« è¿‡æ»¤æ‰: {paper.title[:60]}... (åŒ…å«æ’é™¤è¯: {keyword})")
                    return False
        
        # æ£€æŸ¥å¿…éœ€å…³é”®è¯
        if require_keywords:
            has_required = False
            for keyword in require_keywords:
                if keyword.lower() in content:
                    has_required = True
                    break
            if not has_required:
                print(f"  ğŸš« è¿‡æ»¤æ‰: {paper.title[:60]}... (ç¼ºå°‘å¿…éœ€å…³é”®è¯)")
                return False
        
        return True
        
    def search_all(self, keywords: str, start_date: Optional[str] = None,
                   end_date: Optional[str] = None, sources: List[str] = None,
                   exclude_keywords: List[str] = None, require_keywords: List[str] = None) -> List[Paper]:
        """
        åœ¨æ‰€æœ‰é€‰å®šçš„æœç´¢å¼•æ“ä¸Šæœç´¢
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            sources: è¦æœç´¢çš„æ¥æºåˆ—è¡¨ï¼Œé»˜è®¤å…¨éƒ¨
            exclude_keywords: æ’é™¤å…³é”®è¯åˆ—è¡¨
            require_keywords: å¿…éœ€å…³é”®è¯åˆ—è¡¨
            
        Returns:
            æ‰€æœ‰æœç´¢ç»“æœçš„åˆå¹¶åˆ—è¡¨
        """
        if sources is None:
            sources = list(self.engines.keys())
        
        # ä»Configè·å–è¿‡æ»¤è®¾ç½®
        if exclude_keywords is None:
            exclude_keywords = Config.EXCLUDE_KEYWORDS
        if require_keywords is None:
            require_keywords = Config.REQUIRE_KEYWORDS
        
        enable_filter = Config.ENABLE_SMART_FILTER and (exclude_keywords or require_keywords)
        
        if enable_filter:
            print(f"\nğŸ¯ æ™ºèƒ½è¿‡æ»¤å·²å¯ç”¨:")
            if exclude_keywords:
                print(f"   æ’é™¤å…³é”®è¯: {', '.join(exclude_keywords)}")
            if require_keywords:
                print(f"   å¿…éœ€å…³é”®è¯: {', '.join(require_keywords)}")
            print()
            
        all_papers = []
        
        for source in sources:
            if source in self.engines:
                print(f"æ­£åœ¨æœç´¢ {source}...")
                papers = self.engines[source].search(keywords, start_date, end_date)
                
                # åº”ç”¨æ™ºèƒ½è¿‡æ»¤
                if enable_filter:
                    original_count = len(papers)
                    papers = [p for p in papers if self._filter_paper(p, exclude_keywords, require_keywords)]
                    filtered_count = original_count - len(papers)
                    if filtered_count > 0:
                        print(f"  âœ… è¿‡æ»¤æ‰ {filtered_count} ç¯‡ä¸ç›¸å…³è®ºæ–‡")
                
                all_papers.extend(papers)
                print(f"ä» {source} æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                
        return all_papers


# åˆ›å»ºå…¨å±€æœç´¢ç®¡ç†å™¨å®ä¾‹
search_manager = SearchManager()
