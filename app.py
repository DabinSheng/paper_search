import streamlit as st
from datetime import datetime, timedelta
import os
from config import Config
from search_engines import search_manager
from qwen_client import qwen_client
from download_manager import download_manager
from download_history import download_history
from search_history import search_history


# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å­¦æœ¯æ–‡çŒ®æ£€ç´¢Agent",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .paper-card {
        padding: 20px;
        border-radius: 10px;
        border: 1px solid #ddd;
        margin-bottom: 20px;
        background-color: #f9f9f9;
    }
    .paper-title {
        font-size: 18px;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 10px;
    }
    .paper-meta {
        font-size: 14px;
        color: #666;
        margin-bottom: 10px;
    }
    .paper-abstract {
        font-size: 14px;
        line-height: 1.6;
        margin-top: 10px;
    }
    .translation {
        background-color: #f0f8ff;
        color: #2c3e50;
        padding: 12px;
        border-radius: 8px;
        margin-top: 10px;
        border-left: 4px solid #3498db;
        font-size: 14px;
        line-height: 1.6;
    }
    .translation strong {
        color: #2980b9;
    }
</style>
""", unsafe_allow_html=True)


def init_session_state():
    """åˆå§‹åŒ–session state"""
    if 'search_results' not in st.session_state:
        st.session_state.search_results = []
    if 'selected_papers' not in st.session_state:
        st.session_state.selected_papers = set()
    if 'translations' not in st.session_state:
        st.session_state.translations = {}


def validate_config():
    """éªŒè¯é…ç½®"""
    try:
        Config.validate()
        return True
    except ValueError as e:
        st.error(f"âŒ é…ç½®é”™è¯¯: {str(e)}")
        st.info("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶å¹¶é…ç½® QWEN_API_KEY")
        return False


def perform_search(keywords, start_date, end_date, sources):
    """æ‰§è¡Œæœç´¢"""
    with st.spinner('ğŸ” æ­£åœ¨æœç´¢æ–‡çŒ®...'):
        results = search_manager.search_all(
            keywords=keywords,
            start_date=start_date.strftime('%Y-%m-%d') if start_date else None,
            end_date=end_date.strftime('%Y-%m-%d') if end_date else None,
            sources=sources,
            exclude_keywords=Config.EXCLUDE_KEYWORDS if Config.ENABLE_SMART_FILTER else None,
            require_keywords=Config.REQUIRE_KEYWORDS if Config.ENABLE_SMART_FILTER else None
        )
        st.session_state.search_results = results
        st.session_state.selected_papers = set()
        st.session_state.translations = {}
        
        # ä¿å­˜æœç´¢å†å²
        exclude_keywords_str = ', '.join(Config.EXCLUDE_KEYWORDS) if Config.ENABLE_SMART_FILTER else ""
        search_history.add_search(
            keywords=keywords,
            exclude_keywords=exclude_keywords_str,
            sources=sources,
            results_count=len(results)
        )
    return results


def translate_text(text, cache_key, auto=False):
    """ç¿»è¯‘æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if cache_key not in st.session_state.translations:
        if not auto:
            with st.spinner('ğŸŒ æ­£åœ¨ç¿»è¯‘...'):
                translation = qwen_client.translate_to_chinese(text)
                st.session_state.translations[cache_key] = translation
        else:
            # è‡ªåŠ¨ç¿»è¯‘ï¼ˆåå°é™é»˜ç¿»è¯‘ï¼‰
            translation = qwen_client.translate_to_chinese(text)
            st.session_state.translations[cache_key] = translation
    return st.session_state.translations.get(cache_key)


def auto_translate_papers(papers):
    """è‡ªåŠ¨ç¿»è¯‘è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦"""
    if not Config.AUTO_TRANSLATE:
        return
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    total = len(papers)
    for idx, paper in enumerate(papers):
        paper_dict = paper.to_dict()
        paper_id_base = f"{idx}_{paper_dict['title'][:50]}"
        
        # ç¿»è¯‘æ ‡é¢˜
        title_key = f"title_{paper_id_base}"
        if title_key not in st.session_state.translations:
            translate_text(paper_dict['title'], title_key, auto=True)
        
        # ç¿»è¯‘æ‘˜è¦ï¼ˆå¯é€‰ï¼Œæ‘˜è¦è¾ƒé•¿å¯èƒ½æ¶ˆè€—è¾ƒå¤šé…é¢ï¼‰
        abstract_key = f"abstract_{paper_id_base}"
        if abstract_key not in st.session_state.translations:
            translate_text(paper_dict['abstract'], abstract_key, auto=True)
        
        # æ›´æ–°è¿›åº¦
        progress = (idx + 1) / total
        progress_bar.progress(progress)
        progress_text.text(f"æ­£åœ¨ç¿»è¯‘è®ºæ–‡ {idx + 1}/{total}...")
    
    progress_bar.empty()
    progress_text.empty()


def display_paper(paper, index):
    """æ˜¾ç¤ºå•ç¯‡è®ºæ–‡"""
    paper_dict = paper.to_dict()
    
    # åˆ›å»ºå”¯ä¸€æ ‡è¯†
    paper_id = f"{index}_{paper_dict['title'][:50]}"
    
    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
    is_downloaded = download_history.is_downloaded(paper_dict['title'])
    download_info = download_history.get_download_info(paper_dict['title']) if is_downloaded else None
    
    # å‹¾é€‰æ¡†
    col1, col2 = st.columns([0.05, 0.95])
    with col1:
        # å¦‚æœå·²ä¸‹è½½ï¼Œç¦ç”¨å‹¾é€‰æ¡†
        if is_downloaded:
            st.checkbox(
                "é€‰æ‹©",
                key=f"select_{paper_id}",
                value=False,
                disabled=True,
                label_visibility="collapsed"
            )
        else:
            is_selected = st.checkbox(
                "é€‰æ‹©",
                key=f"select_{paper_id}",
                value=paper_id in st.session_state.selected_papers,
                label_visibility="collapsed"
            )
            if is_selected:
                st.session_state.selected_papers.add(paper_id)
            elif paper_id in st.session_state.selected_papers:
                st.session_state.selected_papers.remove(paper_id)
    
    with col2:
        # æ ‡é¢˜ï¼ˆè‹±æ–‡ï¼‰+ å·²ä¸‹è½½æ ‡è®°
        if is_downloaded:
            st.markdown(f"### ğŸ“„ {paper_dict['title']} âœ…")
            st.markdown(f'<div style="background-color: #d4edda; color: #155724; padding: 8px; border-radius: 5px; margin-bottom: 10px; font-size: 13px;">ğŸ“¥ å·²ä¸‹è½½ | æ—¥æœŸ: {download_info["date_only"]}</div>', 
                       unsafe_allow_html=True)
        else:
            st.markdown(f"### ğŸ“„ {paper_dict['title']}")
        
        # æ˜¾ç¤ºä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚æœå·²ç¿»è¯‘æˆ–ç‚¹å‡»ç¿»è¯‘æŒ‰é’®ï¼‰
        title_key = f"title_{paper_id}"
        title_translation = st.session_state.translations.get(title_key)
        
        if title_translation:
            st.markdown(f'<div class="translation"><strong>ä¸­æ–‡æ ‡é¢˜:</strong> {title_translation}</div>', 
                       unsafe_allow_html=True)
        elif not Config.AUTO_TRANSLATE:
            # å¦‚æœæœªå¼€å¯è‡ªåŠ¨ç¿»è¯‘ï¼Œæ˜¾ç¤ºç¿»è¯‘æŒ‰é’®
            if st.button("ğŸŒ ç¿»è¯‘æ ‡é¢˜", key=f"trans_title_btn_{paper_id}"):
                translation = translate_text(paper_dict['title'], title_key, auto=False)
                if translation:
                    st.rerun()
        
        # å…ƒä¿¡æ¯
        meta_info = []
        if paper_dict['source']:
            meta_info.append(f"ğŸ“š æ¥æº: {paper_dict['source']}")
        if paper_dict['published']:
            meta_info.append(f"ğŸ“… å‘è¡¨æ—¥æœŸ: {paper_dict['published']}")
        if paper_dict['authors']:
            authors_str = ", ".join(paper_dict['authors'][:3])
            if len(paper_dict['authors']) > 3:
                authors_str += f" ç­‰ {len(paper_dict['authors'])} ä½ä½œè€…"
            meta_info.append(f"âœï¸ ä½œè€…: {authors_str}")
        
        if meta_info:
            st.markdown(" | ".join(meta_info))
        
        # é“¾æ¥
        col_link1, col_link2 = st.columns(2)
        with col_link1:
            if paper_dict['url']:
                st.markdown(f"ğŸ”— [æŸ¥çœ‹è®ºæ–‡]({paper_dict['url']})")
        with col_link2:
            if paper_dict['pdf_url']:
                st.markdown(f"ğŸ“¥ [PDFé“¾æ¥]({paper_dict['pdf_url']})")
        
        # æ‘˜è¦
        with st.expander("æŸ¥çœ‹æ‘˜è¦", expanded=False):
            st.markdown(f"**English Abstract:** {paper_dict['abstract']}")
            
            # æ˜¾ç¤ºä¸­æ–‡æ‘˜è¦ï¼ˆå¦‚æœå·²ç¿»è¯‘æˆ–ç‚¹å‡»ç¿»è¯‘æŒ‰é’®ï¼‰
            abstract_key = f"abstract_{paper_id}"
            abstract_translation = st.session_state.translations.get(abstract_key)
            
            if abstract_translation:
                st.markdown("**ä¸­æ–‡æ‘˜è¦:**")
                st.markdown(f'<div class="translation">{abstract_translation}</div>', 
                          unsafe_allow_html=True)
            elif not Config.AUTO_TRANSLATE:
                # å¦‚æœæœªå¼€å¯è‡ªåŠ¨ç¿»è¯‘ï¼Œæ˜¾ç¤ºç¿»è¯‘æŒ‰é’®
                if st.button("ğŸŒ ç¿»è¯‘æ‘˜è¦", key=f"trans_abs_btn_{paper_id}"):
                    translation = translate_text(paper_dict['abstract'], abstract_key, auto=False)
                    if translation:
                        st.rerun()
        
        st.divider()
    
    return paper_dict, paper_id


def download_selected_papers():
    """ä¸‹è½½é€‰ä¸­çš„è®ºæ–‡"""
    if not st.session_state.selected_papers:
        st.warning("âš ï¸ è¯·å…ˆé€‰æ‹©è¦ä¸‹è½½çš„è®ºæ–‡")
        return
    
    # è·å–é€‰ä¸­çš„è®ºæ–‡
    selected_indices = []
    for paper_id in st.session_state.selected_papers:
        try:
            idx = int(paper_id.split('_')[0])
            selected_indices.append(idx)
        except:
            continue
    
    papers_to_download = [
        st.session_state.search_results[idx].to_dict() 
        for idx in selected_indices 
        if idx < len(st.session_state.search_results)
    ]
    
    # è¿‡æ»¤æ‰æ²¡æœ‰PDFé“¾æ¥çš„è®ºæ–‡
    papers_with_pdf = [p for p in papers_to_download if p.get('pdf_url')]
    
    if not papers_with_pdf:
        st.error("âŒ é€‰ä¸­çš„è®ºæ–‡éƒ½æ²¡æœ‰å¯ç”¨çš„PDFé“¾æ¥")
        return
    
    # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    def update_progress(current, total):
        progress = (current + 1) / total
        progress_bar.progress(progress)
        status_text.text(f"æ­£åœ¨ä¸‹è½½: {current + 1}/{total}")
    
    # æ‰§è¡Œä¸‹è½½
    results = download_manager.download_multiple(
        papers_with_pdf,
        progress_callback=update_progress
    )
    
    # æ˜¾ç¤ºç»“æœ
    progress_bar.empty()
    status_text.empty()
    
    # æ˜¾ç¤ºä¸‹è½½ç»Ÿè®¡
    total_success = len(results['success'])
    total_skipped = len(results.get('skipped', []))
    total_failed = len(results['failed'])
    
    if total_success > 0:
        st.success(f"âœ… æˆåŠŸä¸‹è½½ {total_success} ç¯‡è®ºæ–‡")
    
    if total_skipped > 0:
        st.info(f"â„¹ï¸ è·³è¿‡ {total_skipped} ç¯‡å·²ä¸‹è½½çš„è®ºæ–‡")
        with st.expander("æŸ¥çœ‹è·³è¿‡çš„è®ºæ–‡"):
            for item in results['skipped']:
                st.text(f"- {item['title']}: {item['message']}")
    
    if total_failed > 0:
        st.warning(f"âš ï¸ {total_failed} ç¯‡è®ºæ–‡ä¸‹è½½å¤±è´¥")
        with st.expander("æŸ¥çœ‹å¤±è´¥è¯¦æƒ…"):
            for item in results['failed']:
                st.text(f"- {item['title']}: {item['message']}")
    
    st.info(f"ğŸ“ ä¸‹è½½ä½ç½®: {download_manager.get_download_path()}")
    st.info(f"ğŸ“Š å†å²ç»Ÿè®¡: ç´¯è®¡ä¸‹è½½ {download_history.get_total_downloads()} ç¯‡è®ºæ–‡")


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–
    init_session_state()
    
    # æ ‡é¢˜
    st.title("ğŸ“š å­¦æœ¯æ–‡çŒ®æ£€ç´¢Agent")
    st.markdown("åŸºäºQwen APIçš„æ™ºèƒ½æ–‡çŒ®æ£€ç´¢å’Œä¸‹è½½å·¥å…·")
    
    # éªŒè¯é…ç½®
    if not validate_config():
        return
    
    # ä¾§è¾¹æ  - æœç´¢é…ç½®
    with st.sidebar:
        st.header("ğŸ”§ æœç´¢è®¾ç½®")
        
        # æœç´¢å†å²å¿«æ·é€‰æ‹©
        last_search = search_history.get_last_search()
        if last_search:
            with st.expander("ğŸ“œ æœç´¢å†å²", expanded=False):
                st.caption("ç‚¹å‡»å¿«é€Ÿå¡«å……ä¸Šæ¬¡æœç´¢")
                if st.button(f"ğŸ”„ {last_search['keywords']}", key="load_last"):
                    st.session_state['load_keywords'] = last_search['keywords']
                    st.session_state['load_exclude'] = last_search.get('exclude_keywords', '')
                    st.rerun()
                
                # æ˜¾ç¤ºæœ€è¿‘5æ¬¡æœç´¢
                recent = search_history.get_recent_searches(5)
                if len(recent) > 1:
                    st.caption("æœ€è¿‘æœç´¢:")
                    for idx, record in enumerate(recent[1:], 1):
                        col1, col2 = st.columns([0.85, 0.15])
                        with col1:
                            if st.button(
                                f"{record['keywords'][:30]}...", 
                                key=f"history_{idx}",
                                help=f"æ’é™¤è¯: {record.get('exclude_keywords', 'æ— ')}"
                            ):
                                st.session_state['load_keywords'] = record['keywords']
                                st.session_state['load_exclude'] = record.get('exclude_keywords', '')
                                st.rerun()
                        with col2:
                            if st.button("ğŸ—‘ï¸", key=f"del_{idx}", help="åˆ é™¤"):
                                search_history.remove_search(idx)
                                st.rerun()
        
        # å…³é”®è¯è¾“å…¥
        default_keywords = st.session_state.get('load_keywords', '')
        keywords = st.text_input(
            "æœç´¢å…³é”®è¯",
            value=default_keywords,
            placeholder="ä¾‹å¦‚: machine learning, neural networks",
            help="è¾“å…¥è¦æœç´¢çš„å…³é”®è¯"
        )
        
        # æ¸…é™¤åŠ è½½çš„å…³é”®è¯ï¼ˆé¿å…æ¯æ¬¡éƒ½è‡ªåŠ¨å¡«å……ï¼‰
        if 'load_keywords' in st.session_state:
            del st.session_state['load_keywords']
        
        # æ—¥æœŸèŒƒå›´
        st.subheader("ğŸ“… æ—¥æœŸèŒƒå›´")
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=datetime.now() - timedelta(days=3),
                max_value=datetime.now()
            )
        with col2:
            end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=datetime.now(),
                max_value=datetime.now()
            )
        
        # æ•°æ®æºé€‰æ‹©
        st.subheader("ğŸ“– æ•°æ®æº")
        sources = []
        if st.checkbox("ArXiv", value=True):
            sources.append('arxiv')
        if st.checkbox("OpenReview", value=True):
            sources.append('openreview')
        if st.checkbox("Google Scholar", value=False):
            sources.append('google_scholar')
        
        # é«˜çº§è®¾ç½®
        st.subheader("âš™ï¸ é«˜çº§è®¾ç½®")
        
        # æ™ºèƒ½è¿‡æ»¤
        with st.expander("ğŸ¯ æ™ºèƒ½è¿‡æ»¤", expanded=False):
            enable_filter = st.toggle(
                "å¯ç”¨æ™ºèƒ½è¿‡æ»¤",
                value=Config.ENABLE_SMART_FILTER,
                help="è¿‡æ»¤æ‰ä¸ç›¸å…³çš„è®ºæ–‡"
            )
            Config.ENABLE_SMART_FILTER = enable_filter
            
            if enable_filter:
                # å¿«æ·å¡«å……å¸¸ç”¨æ’é™¤è¯
                popular_excludes = search_history.get_popular_excludes(3)
                if popular_excludes:
                    st.caption("å¸¸ç”¨æ’é™¤è¯:")
                    cols = st.columns(len(popular_excludes))
                    for idx, exclude in enumerate(popular_excludes):
                        with cols[idx]:
                            if st.button(f"ğŸ“Œ {exclude}", key=f"pop_ex_{idx}", help="ç‚¹å‡»å¡«å……"):
                                st.session_state['load_exclude'] = exclude
                                st.rerun()
                
                st.markdown("**æ’é™¤å…³é”®è¯** (åŒ…å«è¿™äº›è¯çš„è®ºæ–‡ä¼šè¢«è¿‡æ»¤)")
                default_exclude = st.session_state.get('load_exclude', '\n'.join(Config.EXCLUDE_KEYWORDS))
                exclude_text = st.text_area(
                    "æ’é™¤å…³é”®è¯",
                    value=default_exclude,
                    placeholder="æ¯è¡Œä¸€ä¸ªå…³é”®è¯\nä¾‹å¦‚:\nhardware\nmemory chip\ncircuit",
                    help="è®ºæ–‡æ ‡é¢˜æˆ–æ‘˜è¦ä¸­åŒ…å«è¿™äº›è¯çš„ä¼šè¢«è¿‡æ»¤æ‰",
                    label_visibility="collapsed"
                )
                Config.EXCLUDE_KEYWORDS = [k.strip() for k in exclude_text.split('\n') if k.strip()]
                
                # æ¸…é™¤åŠ è½½çš„æ’é™¤è¯
                if 'load_exclude' in st.session_state:
                    del st.session_state['load_exclude']
                
                st.markdown("**å¿…éœ€å…³é”®è¯** (è‡³å°‘åŒ…å«ä¸€ä¸ªçš„è®ºæ–‡æ‰ä¿ç•™)")
                require_text = st.text_area(
                    "å¿…éœ€å…³é”®è¯",
                    value="\n".join(Config.REQUIRE_KEYWORDS),
                    placeholder="æ¯è¡Œä¸€ä¸ªå…³é”®è¯\nä¾‹å¦‚:\ndeep learning\nneural network\ntransformer",
                    help="è®ºæ–‡å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªè¿™äº›å…³é”®è¯",
                    label_visibility="collapsed"
                )
                Config.REQUIRE_KEYWORDS = [k.strip() for k in require_text.split('\n') if k.strip()]
                
                # æ˜¾ç¤ºå½“å‰è¿‡æ»¤è®¾ç½®
                if Config.EXCLUDE_KEYWORDS or Config.REQUIRE_KEYWORDS:
                    st.markdown("---")
                    if Config.EXCLUDE_KEYWORDS:
                        st.caption(f"ğŸš« æ’é™¤: {', '.join(Config.EXCLUDE_KEYWORDS[:3])}{'...' if len(Config.EXCLUDE_KEYWORDS) > 3 else ''}")
                    if Config.REQUIRE_KEYWORDS:
                        st.caption(f"âœ… å¿…éœ€: {', '.join(Config.REQUIRE_KEYWORDS[:3])}{'...' if len(Config.REQUIRE_KEYWORDS) > 3 else ''}")
        
        # æœ€å¤§ç»“æœæ•°
        max_results = st.slider(
            "æœ€å¤§æœç´¢ç»“æœæ•°",
            min_value=10,
            max_value=200,
            value=Config.MAX_RESULTS,
            step=10,
            help="æ¯æ¬¡æœç´¢è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡"
        )
        Config.MAX_RESULTS = max_results
        
        # è‡ªåŠ¨ç¿»è¯‘å¼€å…³
        auto_translate = st.toggle(
            "è‡ªåŠ¨ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦",
            value=Config.AUTO_TRANSLATE,
            help="å¼€å¯åæœç´¢å®Œæˆè‡ªåŠ¨ç¿»è¯‘æ‰€æœ‰è®ºæ–‡ï¼Œå…³é—­åéœ€æ‰‹åŠ¨ç‚¹å‡»ç¿»è¯‘æŒ‰é’®"
        )
        Config.AUTO_TRANSLATE = auto_translate
        
        # ä¸‹è½½è·¯å¾„è®¾ç½®
        st.subheader("ğŸ“ ä¸‹è½½è®¾ç½®")
        custom_path = st.text_input(
            "ä¸‹è½½è·¯å¾„",
            value=download_manager.get_download_path(),
            help="è®ºæ–‡PDFçš„ä¿å­˜è·¯å¾„"
        )
        if custom_path != download_manager.get_download_path():
            download_manager.set_download_path(custom_path)
        
        # ä¸‹è½½å†å²ç®¡ç†
        st.subheader("ğŸ“Š ä¸‹è½½å†å²")
        total_downloads = download_history.get_total_downloads()
        st.write(f"ç´¯è®¡ä¸‹è½½: **{total_downloads}** ç¯‡è®ºæ–‡")
        
        if total_downloads > 0:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºä¸‹è½½å†å²", use_container_width=True):
                download_history.clear_history()
                st.success("ä¸‹è½½å†å²å·²æ¸…ç©º")
                st.rerun()
        
        # æœç´¢å†å²ç®¡ç†
        st.subheader("ğŸ“œ æœç´¢å†å²ç®¡ç†")
        total_searches = len(search_history.get_recent_searches())
        st.write(f"å†å²æœç´¢: **{total_searches}** æ¡")
        
        if total_searches > 0:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºæœç´¢å†å²", use_container_width=True):
                search_history.clear_history()
                st.success("æœç´¢å†å²å·²æ¸…ç©º")
                st.rerun()
            
            # æŸ¥çœ‹å®Œæ•´å†å²
            with st.expander("æŸ¥çœ‹å®Œæ•´å†å²"):
                for idx, record in enumerate(search_history.get_recent_searches(20)):
                    st.markdown(f"""
                    **{idx+1}. {record['keywords']}**  
                    æ’é™¤è¯: {record.get('exclude_keywords', 'æ— ')}  
                    ç»“æœ: {record.get('results_count', 0)}ç¯‡ | ä½¿ç”¨{record.get('search_count', 1)}æ¬¡  
                    æ—¶é—´: {record.get('last_search_time', '')[:16]}
                    """)
                    st.markdown("---")
        
        # æœç´¢æŒ‰é’®
        st.markdown("---")
        search_button = st.button("ğŸ” å¼€å§‹æœç´¢", type="primary", use_container_width=True)
    
    # ä¸»ç•Œé¢
    if search_button:
        if not keywords:
            st.warning("âš ï¸ è¯·è¾“å…¥æœç´¢å…³é”®è¯")
        elif not sources:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ•°æ®æº")
        else:
            results = perform_search(keywords, start_date, end_date, sources)
            
            if results:
                st.success(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³è®ºæ–‡")
                # è‡ªåŠ¨ç¿»è¯‘æ‰€æœ‰è®ºæ–‡
                if Config.AUTO_TRANSLATE:
                    with st.spinner('ğŸŒ æ­£åœ¨è‡ªåŠ¨ç¿»è¯‘è®ºæ–‡...'):
                        auto_translate_papers(results)
                    st.success("âœ¨ ç¿»è¯‘å®Œæˆï¼")
            else:
                st.info("â„¹ï¸ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯")
    
    # æ˜¾ç¤ºæœç´¢ç»“æœ
    if st.session_state.search_results:
        st.header("ğŸ“‘ æœç´¢ç»“æœ")
        
        # æ˜¾ç¤ºæœç´¢é…ç½®ä¿¡æ¯
        col_info1, col_info2 = st.columns(2)
        with col_info1:
            st.caption(f"ğŸ“Š æœç´¢åˆ° {len(st.session_state.search_results)} ç¯‡è®ºæ–‡")
        with col_info2:
            translate_status = "âœ… å·²å¼€å¯" if Config.AUTO_TRANSLATE else "âŒ å·²å…³é—­"
            st.caption(f"ğŸŒ è‡ªåŠ¨ç¿»è¯‘: {translate_status}")
        
        # æ“ä½œæŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            if st.button("âœ… å…¨é€‰"):
                for idx, paper in enumerate(st.session_state.search_results):
                    paper_dict = paper.to_dict()
                    paper_id = f"{idx}_{paper_dict['title'][:50]}"
                    st.session_state.selected_papers.add(paper_id)
                st.rerun()
        
        with col2:
            if st.button("âŒ å–æ¶ˆå…¨é€‰"):
                st.session_state.selected_papers.clear()
                st.rerun()
        
        st.markdown(f"**å·²é€‰æ‹©: {len(st.session_state.selected_papers)} ç¯‡è®ºæ–‡**")
        
        # æ˜¾ç¤ºæ¯ç¯‡è®ºæ–‡
        for idx, paper in enumerate(st.session_state.search_results):
            display_paper(paper, idx)
        
        # ä¸‹è½½æŒ‰é’®
        st.markdown("---")
        col1, col2, col3 = st.columns([2, 1, 2])
        with col2:
            if st.button("ğŸ“¥ ä¸‹è½½é€‰ä¸­è®ºæ–‡", type="primary", use_container_width=True):
                download_selected_papers()
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #666;'>"
        "Powered by Qwen API | å­¦æœ¯æ–‡çŒ®æ£€ç´¢Agent"
        "</div>",
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
